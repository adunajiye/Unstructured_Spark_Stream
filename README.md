# Unstructured_Spark_Stream


# Introduction
This project serves as a comprehensive guide to building an end-to-end data engineering pipeline using Unstructured_datset, Apache Spark, aws S3, Athena and Data warehouse system Redshift. It covers each stage from data acquisition, processing, Batch streaming with Apache spark, production to aws s3 and setting up its connection.
# Architecture
![Screenshot (70)](https://github.com/adunajiye/Unstructured_Spark_Stream/assets/80220180/a7b2b7b7-bc76-4344-a3bc-ab85dc942aad)


# Technologies

<li>Python</li>
<li>Apache Spark</li>
<li>AWS</li>
<li>Docker</li>

<div class="code-container">
  <button class="copy-button" data-clipboard-target="#example-code">Clone the repository</button>

  ```python
  git clone https://github.com/adunajiye/Unstructured_Spark_Stream.git
 ```

<div class="code-container">
  <button class="copy-button" data-clipboard-target="#example-code">Run Docker Compose to spin up the spark cluste</button>

  ```python
  docker-compose up -d
 ```


